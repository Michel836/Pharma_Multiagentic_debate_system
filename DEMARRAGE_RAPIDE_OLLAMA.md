# ğŸš€ DÃ‰MARRAGE RAPIDE - MODE 100% LOCAL AVEC OLLAMA

## âœ… Version simplifiÃ©e sans connexion externe!

Ce guide vous permet de dÃ©marrer le systÃ¨me multiagent en utilisant **uniquement Ollama** sur votre machine locale, sans aucune connexion Internet ou API externe.

---

## ğŸ“‹ PrÃ©requis

1. **Ollama installÃ©** : [TÃ©lÃ©charger Ollama](https://ollama.ai)
2. **Python 3.11+** installÃ©
3. **Node.js 18+** installÃ©
4. **8 GB RAM minimum** (16 GB recommandÃ©)

---

## ğŸ¯ DÃ©marrage Ultra-Rapide (30 secondes)

### Option 1: Script automatique (Windows)

Double-cliquez sur:
```
start_ollama_only.bat
```

C'est tout! Le script fait tout automatiquement:
- âœ… DÃ©marre Ollama
- âœ… TÃ©lÃ©charge le modÃ¨le llama3.2 (si nÃ©cessaire)
- âœ… Lance le backend Python
- âœ… Lance le frontend React
- âœ… Ouvre votre navigateur

---

### Option 2: DÃ©marrage manuel

#### Ã‰tape 1: DÃ©marrer Ollama
```bash
ollama serve
```

#### Ã‰tape 2: Installer le modÃ¨le (premiÃ¨re fois seulement)
```bash
ollama pull llama3.2
```

#### Ã‰tape 3: DÃ©marrer le backend
```bash
cd backend
python main_ollama.py
```

#### Ã‰tape 4: DÃ©marrer le frontend
```bash
cd frontend
npm start
```

#### Ã‰tape 5: Ouvrir le navigateur
Allez Ã : http://localhost:3000

---

## ğŸ® Utilisation

### Interface Simple

1. **Ã‰crivez votre question** dans le champ de texte
2. **Cliquez sur "DÃ©marrer le dÃ©bat"**
3. **Regardez les 3 experts dÃ©battre** en temps rÃ©el
4. **Le juge synthÃ©tise** Ã  la fin de chaque tour
5. **Consensus atteint** = rÃ©sultat final!

### Exemple de questions:

- "Quelle est la meilleure mÃ©thode pour valider un nouveau mÃ©dicament?"
- "Comment optimiser la formulation d'un comprimÃ© Ã  libÃ©ration prolongÃ©e?"
- "Quels sont les risques de contamination croisÃ©e en production?"

---

## ğŸ”§ Configuration SimplifiÃ©e

### Changer de modÃ¨le Ollama

Ã‰ditez `backend/config/ollama_config.yaml`:
```yaml
ollama:
  default_model: "mistral:7b"  # ou "llama3.2", "qwen2.5:7b"
```

### ModÃ¨les recommandÃ©s par performance:

| ModÃ¨le | Taille | RAM nÃ©cessaire | QualitÃ© | Vitesse |
|--------|--------|----------------|---------|---------|
| llama3.2:3b | 3.2 GB | 4 GB | Bonne | Rapide |
| mistral:7b | 4.1 GB | 8 GB | TrÃ¨s bonne | Moyenne |
| llama3.1:8b | 4.7 GB | 8 GB | Excellente | Moyenne |
| qwen2.5:7b | 4.4 GB | 8 GB | Excellente | Rapide |

### Installer d'autres modÃ¨les:
```bash
ollama pull mistral:7b
ollama pull qwen2.5:7b
```

---

## ğŸ¯ Architecture SimplifiÃ©e

```
Votre PC (100% local)
â”‚
â”œâ”€â”€ Ollama (Port 11434)
â”‚   â””â”€â”€ ModÃ¨le llama3.2
â”‚       â”œâ”€â”€ Expert 1
â”‚       â”œâ”€â”€ Expert 2
â”‚       â”œâ”€â”€ Expert 3
â”‚       â””â”€â”€ Juge
â”‚
â”œâ”€â”€ Backend Python (Port 8000)
â”‚   â””â”€â”€ Orchestrateur de dÃ©bat
â”‚
â””â”€â”€ Frontend React (Port 3000)
    â””â”€â”€ Interface utilisateur
```

---

## ğŸš¨ RÃ©solution de problÃ¨mes

### Ollama ne dÃ©marre pas
```bash
# VÃ©rifier l'installation
ollama version

# RedÃ©marrer le service
ollama serve
```

### ModÃ¨le trop lent
- Utilisez un modÃ¨le plus petit (llama3.2:3b)
- Fermez les autres applications
- RÃ©duisez le nombre de tours de dÃ©bat

### Port dÃ©jÃ  utilisÃ©
```bash
# Changer les ports dans .env.ollama
REACT_APP_API_URL=http://localhost:8001  # Backend sur 8001
```

---

## ğŸ’¡ Astuces Performance

### 1. Pour PC avec peu de RAM (< 8GB)
- Utilisez `llama3.2:3b` ou `phi3:mini`
- Limitez Ã  3 tours de dÃ©bat maximum
- Un seul expert + un juge

### 2. Pour PC Gaming (16GB+ RAM, GPU)
- Utilisez `llama3.1:8b` ou `mistral:7b`
- Jusqu'Ã  10 tours de dÃ©bat
- 3-5 experts possibles

### 3. Optimisation GPU (NVIDIA)
```bash
# Activer l'accÃ©lÃ©ration GPU
set OLLAMA_GPU=true
ollama serve
```

---

## ğŸ“Š Exemples de DÃ©bats

### DÃ©bat Pharmaceutique Simple
**Question**: "Validation d'un nouveau processus de stÃ©rilisation"

**RÃ©sultat typique**:
- Tour 1: Identification des risques
- Tour 2: MÃ©thodes de validation proposÃ©es
- Tour 3: Consensus sur l'approche
- SynthÃ¨se: Plan d'action structurÃ©

### Temps de rÃ©ponse moyens
- ModÃ¨le 3B: ~5 secondes par rÃ©ponse
- ModÃ¨le 7B: ~10 secondes par rÃ©ponse
- DÃ©bat complet (3 tours): 2-3 minutes

---

## ğŸ‰ C'est parti!

1. **Lancez** `start_ollama_only.bat`
2. **Posez** votre question
3. **Regardez** le dÃ©bat
4. **Obtenez** des rÃ©ponses validÃ©es!

**Aucune connexion Internet requise** âœ…
**100% confidentiel sur votre PC** ğŸ”’
**Gratuit et open source** ğŸ’š

---

## ğŸ“ Support

- ProblÃ¨me? CrÃ©ez une issue sur GitHub
- Question? Consultez la FAQ ci-dessous

### FAQ

**Q: Puis-je utiliser d'autres modÃ¨les?**
R: Oui! Tous les modÃ¨les Ollama sont compatibles.

**Q: Combien de RAM ai-je besoin?**
R: Minimum 4GB, recommandÃ© 8GB+

**Q: Est-ce vraiment 100% local?**
R: Oui! Aucune donnÃ©e ne sort de votre PC.

**Q: Puis-je modifier les prompts des agents?**
R: Oui, dans `backend/config/ollama_config.yaml`

---

## ğŸ† Commandes Utiles

```bash
# Voir les modÃ¨les installÃ©s
ollama list

# Supprimer un modÃ¨le
ollama rm llama3.2

# Voir l'utilisation mÃ©moire
ollama ps

# Tester directement un modÃ¨le
ollama run llama3.2 "Bonjour!"
```

---

**Profitez du systÃ¨me multiagent 100% local! ğŸš€**